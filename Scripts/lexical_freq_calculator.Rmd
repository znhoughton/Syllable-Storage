---
title: "Lexical_freq_calculator"
author: "Zachary Houghton"
date: "2024-01-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(reticulate)
library(tidyverse)
```

## Lexical Frequency Calculator

This is just a simple script that will take a list of words and return the list with their lexical frequencies according to google n_grams.

Let's load in our data first.

```{r}
list_of_words = read_csv('word_list.csv')

list_of_words = data.frame(words = c('dog', 'cat'))

r_to_py(list_of_words)
```

Now let's get the lexical frequencies. Note that this will only work if you have downloaded the google 1-grams corpus, which I will not be including in the github due to its size and potential copyright issues.

```{python}

from zs import ZS

def get_lexical_freq(target_word, match_count = True):
    z_1gram = ZS("D:\\PhD Stuff\\Linguistics Stuff\\Google NGRAMS\\google-books-eng-us-all-20120701-1gram.zs")

    word1 = target_word.encode("utf-8")

    match_count_val = 0
    volume_count = 0
    
    for record in z_1gram.search(prefix=word1):  # number of times word1 appears total
        word_decoded = record.decode("utf-8")
        word_split = word_decoded.split("\t")
        corpus_word = word_split[0].split('_')[0]
        if corpus_word == word1.decode('utf-8'):
            #print(word_split)
            
            match_count_val += int(word_split[2])  # match count
            volume_count == int(word_split[3])  # volume count
    if match_count:
        return(match_count_val)
    else:
        return(volume_count)

def get_corpus_size():
    z_1gram = ZS("D:\\PhD Stuff\\Linguistics Stuff\\Google NGRAMS\\google-books-eng-us-all-20120701-1gram.zs")
    
    corpus_size = 0
    
    for i,record in enumerate(z_1gram):
        if i % 20000000 == 0 : #progress bar
            print(record.decode('utf-8'))
        word_decoded = record.decode('utf-8')
        #print(word_decoded) #for debugging
        word_split = word_decoded.split('\t')
        corpus_size += int(word_split[2])
    return(corpus_size)

#one_gram_corpus_size = get_corpus_size()
#print(one_gram_corpus_size)
#1-gram corpus size is 1064909866902
#get_lexical_freq('cantelope') 
```

The size of our corpus is:

Now we simply apply this function to our dataframe:

```{python}
df = r['list_of_words']
one_gram_corpus_size = 1064909866902

df['frequency'] = df['words'].apply(get_lexical_freq)
```
